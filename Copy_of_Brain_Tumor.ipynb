{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxNSKANl3VXVA8oici9+MS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blecyrajan/ML_Projects/blob/main/Copy_of_Brain_Tumor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment & Kaggle Data Setup**"
      ],
      "metadata": {
        "id": "XjUOn2OLJSVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")"
      ],
      "metadata": {
        "id": "SbC74zgWJ_bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing & Augmentation**"
      ],
      "metadata": {
        "id": "XVYHZ-VFKMRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder('/kaggle/input/brain-tumor-mri-dataset/Training', transform=transform)\n",
        "test_dataset = datasets.ImageFolder('/kaggle/input/brain-tumor-mri-dataset/Testing', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "classes = train_dataset.classes\n",
        "print(f\"Classes: {classes}\")\n"
      ],
      "metadata": {
        "id": "h7XZbAWAKor2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Hybrid CNN-ViT Architecture**"
      ],
      "metadata": {
        "id": "xrQP5kAMK_JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class BrainTumorModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        # CNN: ResNet18 for local texture\n",
        "        self.cnn = timm.create_model('resnet18', pretrained=True, features_only=True)\n",
        "        # ViT: Tiny Vision Transformer for global context\n",
        "        self.vit = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Fusion Layer\n",
        "        self.classifier = nn.Linear(512 + 192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Get CNN Features\n",
        "        cnn_maps = self.cnn(x)[-1] # [B, 512, 7, 7]\n",
        "        cnn_feat = torch.mean(cnn_maps, dim=(2, 3)) # Global Average Pool\n",
        "\n",
        "        # 2. Get ViT Features\n",
        "        vit_feat = self.vit(x) # [B, 192]\n",
        "\n",
        "        # 3. Combine\n",
        "        combined = torch.cat((cnn_feat, vit_feat), dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, cnn_maps # Maps used for RSC Loss & Intrinsic Heatmaps\n"
      ],
      "metadata": {
        "id": "a_MGdaYOK-UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSC Loss & Training Logic**"
      ],
      "metadata": {
        "id": "djtX88KVLO7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rsc_loss_fn(feat_orig, feat_flipped):\n",
        "    # Intrinsic Explanation Map Generation\n",
        "    map_orig = torch.mean(feat_orig, dim=1)\n",
        "    map_flipped = torch.mean(feat_flipped, dim=1)\n",
        "    map_flipped_restored = torch.flip(map_flipped, dims=[2])\n",
        "    return nn.functional.mse_loss(map_orig, map_flipped_restored)\n",
        "\n",
        "# Training Settings\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BrainTumorModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # AdamW is better for ViT\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model(epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward Original\n",
        "            logits, feat_orig = model(images)\n",
        "            # Forward Flipped (for Consistency)\n",
        "            _, feat_flipped = model(torch.flip(images, dims=[-1])) # Assuming images are [B, C, H, W], flip along W\n",
        "\n",
        "            # Weighted Loss\n",
        "            loss_cls = criterion(logits, labels)\n",
        "            loss_rsc = rsc_loss_fn(feat_orig, feat_flipped)\n",
        "            total_loss = loss_cls + (0.5 * loss_rsc)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            train_loss += total_loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train_model(epochs=10) # Increase epochs for better accuracy"
      ],
      "metadata": {
        "id": "YJeuPU4nLTLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scientific Evaluation & Plotting**"
      ],
      "metadata": {
        "id": "PBSeEZKyLufd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "def evaluate_and_save_plots():\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out, _ = model(imgs)\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 1. Confusion Matrix Plot\n",
        "    plt.figure(figsize=(10,8))\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix: CNN-ViT RSC Model')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Intrinsic Heatmap Plot\n",
        "    sample_img, _ = test_dataset[10] # Pick a sample\n",
        "    logits, maps = model(sample_img.unsqueeze(0).to(device))\n",
        "\n",
        "    # Process heatmap from the internal learned layer\n",
        "    heatmap = torch.mean(maps, dim=1).squeeze().cpu().detach().numpy()\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "    heatmap = cv2.resize(heatmap, (224, 224))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(sample_img.permute(1, 2, 0).cpu())\n",
        "    plt.title(\"Original MRI\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(heatmap, cmap='jet')\n",
        "    plt.title(\"Intrinsic RSC Explanation\")\n",
        "    plt.savefig(\"explanation_results.png\")\n",
        "    plt.show()\n",
        "\n",
        "evaluate_and_save_plots()"
      ],
      "metadata": {
        "id": "v4Xw3yLtL8Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving for Production**"
      ],
      "metadata": {
        "id": "TlB5pksdNFER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model (weights and architecture)\n",
        "torch.save(model.state_dict(), 'brain_tumor_rsc_model.pth')\n",
        "\n",
        "# To load in Backend:\n",
        "# model = BrainTumorModel()\n",
        "# model.load_state_dict(torch.load('brain_tumor_rsc_model.pth'))\n",
        "# model.eval()\n"
      ],
      "metadata": {
        "id": "jj8uuA3CNJv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Distribution & Sample Visualization**"
      ],
      "metadata": {
        "id": "9z7jXCcuanTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Plot A: Dataset Distribution ---\n",
        "def plot_dataset_summary(dataset):\n",
        "    targets = np.array(dataset.targets)\n",
        "    class_counts = [np.sum(targets == i) for i in range(len(dataset.classes))]\n",
        "\n",
        "    plt.figure(figsize=(10, 6), dpi=100)\n",
        "    sns.barplot(x=dataset.classes, y=class_counts, palette=\"magma\")\n",
        "    plt.title(\"Distribution of Brain Tumor Classes in Dataset\", fontsize=14)\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.savefig(\"figure_1_distribution.png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# --- Plot B: Samples from Each Class ---\n",
        "def plot_class_samples(dataset):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for i, class_name in enumerate(dataset.classes):\n",
        "        # Find first index of this class\n",
        "        idx = dataset.targets.index(i)\n",
        "        img, _ = dataset[idx]\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        # Un-normalize for display\n",
        "        img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "        axes[i].imshow(np.clip(img, 0, 1))\n",
        "        axes[i].set_title(f\"Class: {class_name}\", fontsize=12)\n",
        "        axes[i].axis('off')\n",
        "    plt.savefig(\"figure_2_samples.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_dataset_summary(train_dataset)\n",
        "plot_class_samples(train_dataset)\n"
      ],
      "metadata": {
        "id": "UfqIWZ17agVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Analysis (Before & After)**"
      ],
      "metadata": {
        "id": "9kkseWFIax87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_preprocessing_effect(dataset_path):\n",
        "    from PIL import Image\n",
        "    raw_img = Image.open(train_dataset.imgs[0][0]) # Load one raw file\n",
        "\n",
        "    # Process it using  transform\n",
        "    processed_img, _ = train_dataset[0]\n",
        "    processed_img = processed_img.permute(1, 2, 0).numpy()\n",
        "    processed_img = (processed_img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(raw_img)\n",
        "    plt.title(\"Before: Raw MRI (Varying Size/Intensity)\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(np.clip(processed_img, 0, 1))\n",
        "    plt.title(\"After: Preprocessed (224x224 & Normalized)\")\n",
        "    plt.savefig(\"figure_3_preprocessing.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_preprocessing_effect('brain_data/Training')\n"
      ],
      "metadata": {
        "id": "gI0sxY52a0-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training History (Loss & Accuracy)**"
      ],
      "metadata": {
        "id": "VzSktnSBa5ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss Curve\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    ax1.set_title('Training vs. Validation Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Accuracy Curve\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Acc')\n",
        "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Acc')\n",
        "    ax2.set_title('Training vs. Validation Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.savefig(\"figure_4_history.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GqPiSEwza9ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Advanced Evaluation (Confusion Matrix & ROC)**"
      ],
      "metadata": {
        "id": "IuVrK2KLbR12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "def plot_advanced_metrics(model, loader, classes):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs, _ = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"figure_5_cm.png\")\n",
        "\n",
        "    # --- ROC / AUC ---\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    all_probs = np.array(all_probs)\n",
        "    for i in range(len(classes)):\n",
        "        fpr, tpr, _ = roc_curve(np.array(all_labels) == i, all_probs[:, i])\n",
        "        plt.plot(fpr, tpr, label=f'{classes[i]} (AUC = {auc(fpr, tpr):.3f})')\n",
        "    plt.plot(,, 'k--')\n",
        "    plt.title(\"Multiclass ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"figure_6_roc.png\")\n",
        "\n",
        "plot_advanced_metrics(model, test_loader, classes)\n"
      ],
      "metadata": {
        "id": "l07BESB7bTjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b257fcab"
      },
      "source": [
        "def rsc_loss_fn(feat_orig, feat_flipped):\n",
        "    # Intrinsic Explanation Map Generation\n",
        "    map_orig = torch.mean(feat_orig, dim=1)\n",
        "    map_flipped = torch.mean(feat_flipped, dim=1)\n",
        "    map_flipped_restored = torch.flip(map_flipped, dims=[2])\n",
        "    return nn.functional.mse_loss(map_orig, map_flipped_restored)\n",
        "\n",
        "# Training Settings\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BrainTumorModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # AdamW is better for ViT\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model(epochs=10, train_loader=train_loader, test_loader=test_loader):\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward Original\n",
        "            logits, feat_orig = model(images)\n",
        "            # Forward Flipped (for Consistency)\n",
        "            _, feat_flipped = model(torch.flip(images, dims=[-1])) # Assuming images are [B, C, H, W], flip along W\n",
        "\n",
        "            # Weighted Loss\n",
        "            loss_cls = criterion(logits, labels)\n",
        "            loss_rsc = rsc_loss_fn(feat_orig, feat_flipped)\n",
        "            total_loss = loss_cls + (0.5 * loss_rsc)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_train_loss += total_loss.item()\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits, _ = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = torch.max(logits.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(test_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train_model(epochs=10) # Increase epochs for better accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2812eb85"
      },
      "source": [
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss Curve\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    ax1.set_title('Training vs. Validation Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Accuracy Curve\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Acc')\n",
        "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Acc')\n",
        "    ax2.set_title('Training vs. Validation Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.savefig(\"figure_4_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b289e0b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "\n",
        "def plot_advanced_metrics(model, loader, classes):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs, _ = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # --- Classification Report ---\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=classes))\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"figure_5_cm.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- ROC / AUC ---\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    all_probs = np.array(all_probs)\n",
        "    for i in range(len(classes)):\n",
        "        fpr, tpr, _ = roc_curve(np.array(all_labels) == i, all_probs[:, i])\n",
        "        plt.plot(fpr, tpr, label=f'{classes[i]} (AUC = {auc(fpr, tpr):.3f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # Fixed: added [0,1], [0,1]\n",
        "    plt.title(\"Multiclass ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"figure_6_roc.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_advanced_metrics(model, test_loader, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0736b4bc"
      },
      "source": [
        "# Task\n",
        "The final goal is to develop a robust brain tumor MRI classification system using a hybrid CNN-ViT architecture and Radiological Symmetry Loss (RSC). This involves downloading and setting up the \"masoudnickparvar/brain-tumor-mri-dataset\", visualizing dataset characteristics (class distribution and sample images), defining preprocessing steps including resizing and normalization, and illustrating their effect. The core of the task is defining and training a `BrainTumorModel` that combines ResNet18 for local features and ViT Tiny for global context, along with a custom `rsc_loss_fn` to enhance interpretability. The model will be trained over 10 epochs, with training history (loss and accuracy) visualized to monitor performance. Finally, a comprehensive evaluation will be conducted on the test set, including a classification report, confusion matrix, multi-class ROC curves with AUC scores, and intrinsic heatmaps for explainability, before saving the trained model to 'brain_tumor_rsc_model.pth'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6caa42d"
      },
      "source": [
        "## Dataset Characteristics Visualization\n",
        "\n",
        "### Subtask:\n",
        "Plot the distribution of classes within the training dataset, showing the number of images for each tumor type. Additionally, visualize sample images from each class to provide a visual understanding of the dataset's content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de869d13"
      },
      "source": [
        "**Reasoning**:\n",
        "The user explicitly asked to run the code in the cell that defines the `plot_dataset_summary` and `plot_class_samples` functions. This will generate and display the required visualizations for dataset characteristics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "382d2fef"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Plot A: Dataset Distribution ---\n",
        "def plot_dataset_summary(dataset):\n",
        "    targets = np.array(dataset.targets)\n",
        "    class_counts = [np.sum(targets == i) for i in range(len(dataset.classes))]\n",
        "\n",
        "    plt.figure(figsize=(10, 6), dpi=100)\n",
        "    sns.barplot(x=dataset.classes, y=class_counts, palette=\"magma\")\n",
        "    plt.title(\"Distribution of Brain Tumor Classes in Dataset\", fontsize=14)\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.savefig(\"figure_1_distribution.png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# --- Plot B: Samples from Each Class ---\n",
        "def plot_class_samples(dataset):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for i, class_name in enumerate(dataset.classes):\n",
        "        # Find first index of this class\n",
        "        idx = dataset.targets.index(i)\n",
        "        img, _ = dataset[idx]\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        # Un-normalize for display\n",
        "        img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "        axes[i].imshow(np.clip(img, 0, 1))\n",
        "        axes[i].set_title(f\"Class: {class_name}\", fontsize=12)\n",
        "        axes[i].axis('off')\n",
        "    plt.savefig(\"figure_2_samples.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_dataset_summary(train_dataset)\n",
        "plot_class_samples(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103b84ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `train_dataset` was not defined in the scope of the plotting functions. I will include the necessary data loading and transformation code from earlier cells to ensure `train_dataset` is properly initialized before being used by the plotting functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b52ef317"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Re-initialize dataset and transform for this block\n",
        "IMAGE_SIZE = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder('/kaggle/input/brain-tumor-mri-dataset/Training', transform=transform)\n",
        "\n",
        "# --- Plot A: Dataset Distribution ---\n",
        "def plot_dataset_summary(dataset):\n",
        "    targets = np.array(dataset.targets)\n",
        "    class_counts = [np.sum(targets == i) for i in range(len(dataset.classes))]\n",
        "\n",
        "    plt.figure(figsize=(10, 6), dpi=100)\n",
        "    sns.barplot(x=dataset.classes, y=class_counts, palette=\"magma\")\n",
        "    plt.title(\"Distribution of Brain Tumor Classes in Dataset\", fontsize=14)\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.savefig(\"figure_1_distribution.png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# --- Plot B: Samples from Each Class ---\n",
        "def plot_class_samples(dataset):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for i, class_name in enumerate(dataset.classes):\n",
        "        # Find first index of this class\n",
        "        # Need to handle case where dataset.targets is a list of integers, not an object with an 'index' method.\n",
        "        # Instead, iterate to find an image for the current class.\n",
        "        idx = -1\n",
        "        for j in range(len(dataset)):\n",
        "            if dataset.targets[j] == i:\n",
        "                idx = j\n",
        "                break\n",
        "        if idx != -1:\n",
        "            img, _ = dataset[idx]\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "            # Un-normalize for display\n",
        "            img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "            axes[i].imshow(np.clip(img, 0, 1))\n",
        "            axes[i].set_title(f\"Class: {class_name}\", fontsize=12)\n",
        "            axes[i].axis('off')\n",
        "        else:\n",
        "            # Handle case where a class might not have any samples (unlikely for train_dataset)\n",
        "            axes[i].set_title(f\"Class: {class_name} (No samples)\", fontsize=12)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    plt.savefig(\"figure_2_samples.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_dataset_summary(train_dataset)\n",
        "plot_class_samples(train_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70b2c34"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `FileNotFoundError` because the dataset path was hardcoded and not dynamically referencing the location where `kagglehub.dataset_download` extracts the data. I need to re-execute the `kagglehub.dataset_download` command to get the correct base path and then construct the full path for `ImageFolder` using `os.path.join`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d862a7b2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Re-obtain the dataset path from kagglehub\n",
        "path_to_dataset_root = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
        "\n",
        "# Define the full path to the Training directory\n",
        "train_data_dir = os.path.join(path_to_dataset_root, 'Training')\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_data_dir, transform=transform)\n",
        "\n",
        "# --- Plot A: Dataset Distribution ---\n",
        "def plot_dataset_summary(dataset):\n",
        "    targets = np.array(dataset.targets)\n",
        "    class_counts = [np.sum(targets == i) for i in range(len(dataset.classes))]\n",
        "\n",
        "    plt.figure(figsize=(10, 6), dpi=100)\n",
        "    sns.barplot(x=dataset.classes, y=class_counts, palette=\"magma\")\n",
        "    plt.title(\"Distribution of Brain Tumor Classes in Dataset\", fontsize=14)\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.savefig(\"figure_1_distribution.png\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# --- Plot B: Samples from Each Class ---\n",
        "def plot_class_samples(dataset):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for i, class_name in enumerate(dataset.classes):\n",
        "        idx = -1\n",
        "        # Iterate through the dataset to find an image for the current class\n",
        "        for j in range(len(dataset)):\n",
        "            if dataset.targets[j] == i:\n",
        "                idx = j\n",
        "                break\n",
        "        if idx != -1:\n",
        "            img, _ = dataset[idx]\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "            # Un-normalize for display\n",
        "            img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "            axes[i].imshow(np.clip(img, 0, 1))\n",
        "            axes[i].set_title(f\"Class: {class_name}\", fontsize=12)\n",
        "            axes[i].axis('off')\n",
        "        else:\n",
        "            axes[i].set_title(f\"Class: {class_name} (No samples)\", fontsize=12)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    plt.savefig(\"figure_2_samples.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_dataset_summary(train_dataset)\n",
        "plot_class_samples(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7d93695"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code execution successfully generated the required dataset distribution and sample visualization plots, fulfilling the current subtask. The next logical step in the overall task is to address preprocessing analysis, which involves defining and running the `plot_preprocessing_effect` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96959c05"
      },
      "source": [
        "def plot_preprocessing_effect(dataset_path):\n",
        "    from PIL import Image\n",
        "    raw_img = Image.open(train_dataset.imgs[0][0]) # Load one raw file\n",
        "\n",
        "    # Process it using  transform\n",
        "    processed_img, _ = train_dataset[0]\n",
        "    processed_img = processed_img.permute(1, 2, 0).numpy()\n",
        "    processed_img = (processed_img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(raw_img)\n",
        "    plt.title(\"Before: Raw MRI (Varying Size/Intensity)\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(np.clip(processed_img, 0, 1))\n",
        "    plt.title(\"After: Preprocessed (224x224 & Normalized)\")\n",
        "    plt.savefig(\"figure_3_preprocessing.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_preprocessing_effect('brain_data/Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf2c3e3"
      },
      "source": [
        "## Data Preprocessing and Augmentation Definition\n",
        "\n",
        "### Subtask:\n",
        "Define the `torchvision.transforms` for image preprocessing, including resizing, converting to PyTorch tensors, and normalization. Create `ImageFolder` datasets for training and testing, and set up `DataLoader` instances for efficient batch processing. Extract the class names from the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5036ec8b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the preprocessing steps, create the training and testing datasets and data loaders, and extract class names as per the subtask. This involves using the previously defined `transform` and `path_to_dataset_root`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe406a1a"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming path_to_dataset_root and transform are already defined from previous steps\n",
        "# If not, they would need to be re-initialized here for self-contained execution.\n",
        "# For clarity, let's re-define them if they were not explicitly carried over.\n",
        "\n",
        "# Re-initialize path_to_dataset_root if this cell is meant to be run independently\n",
        "# path_to_dataset_root = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define paths for training and testing datasets\n",
        "train_data_dir = os.path.join(path_to_dataset_root, 'Training')\n",
        "test_data_dir = os.path.join(path_to_dataset_root, 'Testing')\n",
        "\n",
        "# Create ImageFolder datasets\n",
        "train_dataset = datasets.ImageFolder(train_data_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(test_data_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Extract class names\n",
        "classes = train_dataset.classes\n",
        "print(f\"Classes: {classes}\")\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2e82b9"
      },
      "source": [
        "## Hybrid CNN-ViT Architecture Definition\n",
        "\n",
        "### Subtask:\n",
        "Define the `BrainTumorModel` class. This architecture combines a pre-trained ResNet18 as a Convolutional Neural Network (CNN) for local feature extraction and a pre-trained Vision Transformer (ViT Tiny) for capturing global contextual information. Explain the fusion layer that combines features from both and the final linear classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47cb2b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `BrainTumorModel` class, which involves importing `nn` and `timm`, and then implementing the `__init__` and `forward` methods as specified in the instructions to combine CNN and ViT features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "037c5131"
      },
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch # Ensure torch is imported for operations like torch.cat and torch.mean\n",
        "\n",
        "class BrainTumorModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        # CNN: ResNet18 for local texture\n",
        "        # timm.create_model with features_only=True returns a FeatureInfo object\n",
        "        # The output of self.cnn(x) will be a list of feature maps\n",
        "        self.cnn = timm.create_model('resnet18', pretrained=True, features_only=True, out_indices=[4]) # Get only the last feature map\n",
        "\n",
        "        # ViT: Tiny Vision Transformer for global context\n",
        "        # num_classes=0 ensures that the final classification head is removed, returning features\n",
        "        self.vit = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)\n",
        "\n",
        "        # Fusion Layer\n",
        "        # ResNet18 (last feature map, after Global Average Pool) output size is 512\n",
        "        # ViT Tiny output size is 192\n",
        "        self.classifier = nn.Linear(512 + 192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Get CNN Features\n",
        "        # The output of self.cnn(x) is a list, we take the last feature map\n",
        "        cnn_maps = self.cnn(x)[0] # [B, 512, H_feat, W_feat], e.g., [B, 512, 7, 7]\n",
        "        cnn_feat = torch.mean(cnn_maps, dim=(2, 3)) # Global Average Pool -> [B, 512]\n",
        "\n",
        "        # 2. Get ViT Features\n",
        "        vit_feat = self.vit(x) # [B, 192]\n",
        "\n",
        "        # 3. Combine\n",
        "        combined = torch.cat((cnn_feat, vit_feat), dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, cnn_maps # Maps used for RSC Loss & Intrinsic Heatmaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c095af"
      },
      "source": [
        "## Radiological Symmetry Loss (RSC) and Training Logic Definition\n",
        "\n",
        "### Subtask:\n",
        "Define the `rsc_loss_fn` which stands for Radiological Symmetry Loss. This function calculates the Mean Squared Error between the feature map of an original image and the restored feature map of its horizontally flipped counterpart, promoting robust and interpretable representations. Implement the `train_model` function that combines CrossEntropyLoss for classification and the RSC loss, tracking both training and validation loss and accuracy over epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65fc5d64"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `rsc_loss_fn` and the `train_model` functions. This code block will implement both functions as per the instructions, including the training and validation loops, loss calculations (classification and RSC), and tracking of metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcd3d845"
      },
      "source": [
        "def rsc_loss_fn(feat_orig, feat_flipped):\n",
        "    # Intrinsic Explanation Map Generation\n",
        "    map_orig = torch.mean(feat_orig, dim=1)\n",
        "    map_flipped = torch.mean(feat_flipped, dim=1)\n",
        "    map_flipped_restored = torch.flip(map_flipped, dims=[2])\n",
        "    return nn.functional.mse_loss(map_orig, map_flipped_restored)\n",
        "\n",
        "# Training Settings\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BrainTumorModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # AdamW is better for ViT\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model(epochs=10, train_loader=train_loader, test_loader=test_loader):\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward Original\n",
        "            logits, feat_orig = model(images)\n",
        "            # Forward Flipped (for Consistency)\n",
        "            _, feat_flipped = model(torch.flip(images, dims=[-1])) # Assuming images are [B, C, H, W], flip along W\n",
        "\n",
        "            # Weighted Loss\n",
        "            loss_cls = criterion(logits, labels)\n",
        "            loss_rsc = rsc_loss_fn(feat_orig, feat_flipped)\n",
        "            total_loss = loss_cls + (0.5 * loss_rsc)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_train_loss += total_loss.item()\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits, _ = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = torch.max(logits.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(test_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train_model(epochs=10) # Increase epochs for better accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de61c001"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code execution successfully trained the model. The next step in the overall task is to perform scientific evaluation and plotting. The `evaluate_and_save_plots` function, which includes intrinsic heatmap generation for explainability, is the next logical component to execute to fulfill the evaluation requirements of the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea3cbbbe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "def evaluate_and_save_plots():\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out, _ = model(imgs)\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 1. Confusion Matrix Plot\n",
        "    plt.figure(figsize=(10,8))\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix: CNN-ViT RSC Model')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Intrinsic Heatmap Plot\n",
        "    sample_img, _ = test_dataset[10] # Pick a sample\n",
        "    logits, maps = model(sample_img.unsqueeze(0).to(device))\n",
        "\n",
        "    # Process heatmap from the internal learned layer\n",
        "    heatmap = torch.mean(maps, dim=1).squeeze().cpu().detach().numpy()\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "    heatmap = cv2.resize(heatmap, (224, 224))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(sample_img.permute(1, 2, 0).cpu())\n",
        "    plt.title(\"Original MRI\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(heatmap, cmap='jet')\n",
        "    plt.title(\"Intrinsic RSC Explanation\")\n",
        "    plt.savefig(\"explanation_results.png\")\n",
        "    plt.show()\n",
        "\n",
        "evaluate_and_save_plots()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbd87e89"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `rsc_loss_fn` and the `train_model` functions. This code block will implement both functions as per the instructions, including the training and validation loops, loss calculations (classification and RSC), and tracking of metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99780012"
      },
      "source": [
        "def rsc_loss_fn(feat_orig, feat_flipped):\n",
        "    # Intrinsic Explanation Map Generation\n",
        "    map_orig = torch.mean(feat_orig, dim=1)\n",
        "    map_flipped = torch.mean(feat_flipped, dim=1)\n",
        "    map_flipped_restored = torch.flip(map_flipped, dims=[2])\n",
        "    return nn.functional.mse_loss(map_orig, map_flipped_restored)\n",
        "\n",
        "# Training Settings\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BrainTumorModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # AdamW is better for ViT\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model(epochs=10, train_loader=train_loader, test_loader=test_loader):\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward Original\n",
        "            logits, feat_orig = model(images)\n",
        "            # Forward Flipped (for Consistency)\n",
        "            _, feat_flipped = model(torch.flip(images, dims=[-1])) # Assuming images are [B, C, H, W], flip along W\n",
        "\n",
        "            # Weighted Loss\n",
        "            loss_cls = criterion(logits, labels)\n",
        "            loss_rsc = rsc_loss_fn(feat_orig, feat_flipped)\n",
        "            total_loss = loss_cls + (0.5 * loss_rsc)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_train_loss += total_loss.item()\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits, _ = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = torch.max(logits.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(test_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train_model(epochs=10) # Increase epochs for better accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bf068e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code execution successfully trained the model and captured the training history. The next step is to visualize this training history (loss and accuracy) using the `plot_training_history` function, which is a specific requirement of the overall task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ec3d59"
      },
      "source": [
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss Curve\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    ax1.set_title('Training vs. Validation Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Accuracy Curve\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Acc')\n",
        "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Acc')\n",
        "    ax2.set_title('Training vs. Validation Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.savefig(\"figure_4_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2f9e209"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully plotted the training history. Now, as per the overall task, the next step is to conduct advanced evaluation, which includes generating a classification report, confusion matrix, and multi-class ROC curves. The `plot_advanced_metrics` function is designed to perform these evaluations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6e3e850"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "\n",
        "def plot_advanced_metrics(model, loader, classes):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs, _ = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # --- Classification Report ---\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=classes))\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"figure_5_cm.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- ROC / AUC ---\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    all_probs = np.array(all_probs)\n",
        "    for i in range(len(classes)):\n",
        "        fpr, tpr, _ = roc_curve(np.array(all_labels) == i, all_probs[:, i])\n",
        "        plt.plot(fpr, tpr, label=f'{classes[i]} (AUC = {auc(fpr, tpr):.3f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # Fixed: added [0,1], [0,1]\n",
        "    plt.title(\"Multiclass ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"figure_6_roc.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_advanced_metrics(model, test_loader, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2992f7c"
      },
      "source": [
        "## Model Saving\n",
        "\n",
        "### Subtask:\n",
        "Save the trained model's state dictionary to a file named 'brain_tumor_rsc_model.pth'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ab95f3f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask explicitly asks to save the trained model's state dictionary to a file named 'brain_tumor_rsc_model.pth'. This is achieved using `torch.save(model.state_dict(), 'brain_tumor_rsc_model.pth')`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eb213a1"
      },
      "source": [
        "torch.save(model.state_dict(), 'brain_tumor_rsc_model.pth')\n",
        "\n",
        "print(\"Model state dictionary saved to 'brain_tumor_rsc_model.pth'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11c65589"
      },
      "source": [
        "## Preprocessing Impact Visualization\n",
        "\n",
        "### Subtask:\n",
        "Illustrate the effect of the preprocessing steps by displaying a raw MRI image alongside its transformed version (resized to 224x224 and normalized), demonstrating the visual changes applied before feeding to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a49e9f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `plot_preprocessing_effect` function as provided in the notebook context and then call it with the `train_data_dir` variable to illustrate the preprocessing steps. This will fulfill the current subtask requirement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9a40465"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_preprocessing_effect(dataset_path):\n",
        "    # train_dataset and transform are already defined in previous steps.\n",
        "    # Load one raw file. Assumes train_dataset.imgs[0][0] holds the path to the first image.\n",
        "    raw_img = Image.open(train_dataset.imgs[0][0])\n",
        "\n",
        "    # Process it using the defined transform\n",
        "    processed_img, _ = train_dataset[0]\n",
        "    processed_img = processed_img.permute(1, 2, 0).numpy()\n",
        "    # Un-normalize for display\n",
        "    processed_img = (processed_img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(raw_img)\n",
        "    plt.title(\"Before: Raw MRI (Varying Size/Intensity)\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(np.clip(processed_img, 0, 1))\n",
        "    plt.title(\"After: Preprocessed (224x224 & Normalized)\")\n",
        "    plt.savefig(\"figure_3_preprocessing.png\")\n",
        "    plt.show()\n",
        "\n",
        "plot_preprocessing_effect(train_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a060ac"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire workflow, including dataset setup, model training, comprehensive evaluation, and model saving, along with the generated visualizations and their insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f859270a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Overview**: The \"masoudnickparvar/brain-tumor-mri-dataset\" was utilized, containing four classes: 'glioma', 'meningioma', 'notumor', and 'pituitary'. The training set has 5712 images, and the testing set has 1311 images.\n",
        "*   **Class Distribution**: An imbalanced class distribution was observed in the training dataset, with 'Glioma' having the highest number of samples and 'Meningioma' having the lowest.\n",
        "*   **Preprocessing Impact**: Raw MRI images, which varied in size and intensity, were successfully preprocessed. This involved resizing them to 224x224 pixels and normalizing pixel values, visually demonstrating the standardized input fed to the model.\n",
        "*   **Model Architecture**: A hybrid `BrainTumorModel` was developed, combining a pre-trained ResNet18 for local feature extraction (producing 512 features) and a pre-trained ViT Tiny for global contextual understanding (producing 192 features). These features were concatenated into a 704-dimensional vector before being passed to a linear classifier for 4-class prediction.\n",
        "*   **Radiological Symmetry Loss (RSC)**: A custom `rsc_loss_fn` was implemented, calculating the Mean Squared Error between the mean feature map of an original image and the restored feature map of its horizontally flipped counterpart, promoting robust and interpretable representations.\n",
        "*   **Training Performance**: The model was trained for 10 epochs using a combined loss of `CrossEntropyLoss` and `0.5 * rsc_loss_fn`.\n",
        "    *   Initial training accuracy (Epoch 1) was around 89-90%, with validation accuracy around 95-96%.\n",
        "    *   By the final epoch (Epoch 10), training accuracy reached 99-100%, and validation accuracy stabilized impressively at 97-99%.\n",
        "    *   Both training and validation losses decreased significantly over the epochs, indicating effective learning.\n",
        "*   **Comprehensive Evaluation**:\n",
        "    *   The model achieved an overall accuracy of 98% on the test set, demonstrating high precision, recall, and f1-scores across all tumor classes.\n",
        "    *   A confusion matrix confirmed the model's strong performance in differentiating between tumor types and healthy (notumor) cases.\n",
        "    *   Multi-class ROC curves with high AUC scores for each class highlighted the model's robust discriminative capabilities.\n",
        "*   **Model Persistence**: The trained model's state dictionary was successfully saved as 'brain_tumor_rsc_model.pth'.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The hybrid CNN-ViT architecture, augmented with Radiological Symmetry Loss, demonstrates excellent performance in brain tumor classification. The high validation accuracy and strong evaluation metrics suggest the model is well-suited for medical image analysis tasks requiring both local detail and global context.\n",
        "*   Further investigation into the `cnn_maps` output (intended for intrinsic heatmaps) could provide valuable interpretability insights, which was a core goal of integrating the RSC loss. This would involve generating and analyzing these heatmaps to understand which regions of the MRI scans the model focuses on for its predictions.\n"
      ]
    }
  ]
}